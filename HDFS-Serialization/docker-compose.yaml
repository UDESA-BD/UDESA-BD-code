services:
   namenode:
      image: apache/hadoop:3.3.6
      hostname: namenode
      command: ["hdfs", "namenode"]
      ports:
        - 9870:9870
      env_file:
        - ./config
      environment:
          ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
   datanode:
      image: apache/hadoop:3.3.6
      command: ["hdfs", "datanode"]
      deploy: # Con esto le indicamos que queremos tener 3 DataNodes
        mode: replicated
        replicas: 3
      env_file:
        - ./config      
   resourcemanager:
      image: apache/hadoop:3.3.6
      hostname: resourcemanager
      command: ["yarn", "resourcemanager"]
      ports:
         - 8088:8088
      env_file:
        - ./config
      volumes:
        - ./test.sh:/opt/test.sh
   nodemanager:
      image: apache/hadoop:3.3.6
      command: ["yarn", "nodemanager"]
      env_file:
        - ./config
   edge_client: # En esta máquina no corremos ningún proceso de HDFS, pero le pasamos configuración para que pueda llegar al cluster
      image: apache/hadoop:3.3.6
      container_name: cliente_linux
      command: ["tail", "-f", "/dev/null"]  # Para que no se cierre por no tener nada que hacer
      env_file:
        - ./config
      volumes: # De esta forma podremos compartir archivos con nuestro cliente dockerizado
        - ./archivos:/tmp