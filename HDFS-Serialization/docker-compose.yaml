services:
   namenode:
      image: apache/hadoop:3.3.6
      hostname: namenode
      command: ["hdfs", "namenode"]
      ports:
        - 9870:9870 # Interfaz web del NameNode, para ver el estado
        - 8020:8020 # Puerto de la WebHDFS REST API (cuidado con la seguridad!)
      env_file:
        - ./config
      environment:
          ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
   datanode:
      image: apache/hadoop:3.3.6
      command: ["hdfs", "datanode"]
      deploy: # Con esto le indicamos que queremos tener 3 DataNodes
        mode: replicated
        replicas: 3
      env_file:
        - ./config      
   resourcemanager:
      image: apache/hadoop:3.3.6
      hostname: resourcemanager
      command: ["yarn", "resourcemanager"]
      ports:
         - 8088:8088
      env_file:
        - ./config
      volumes:
        - ./test.sh:/opt/test.sh
   nodemanager:
      image: apache/hadoop:3.3.6
      command: ["yarn", "nodemanager"]
      env_file:
        - ./config
   edgenode: # En esta máquina no corremos ningún proceso de HDFS, pero le pasamos configuración para que pueda llegar al cluster
      build: 
         context: .
         dockerfile: Dockerfile_EdgeNode
      command: ["tail", "-f", "/dev/null"]  # Para que no se cierre por no tener nada que hacer
      volumes: # De esta forma podremos compartir archivos con nuestro cliente dockerizado
        - ./archivos:/tmp
   cliente:
      build: 
         context: .
         dockerfile: Dockerfile_Cliente
      command: ["tail", "-f", "/dev/null"]
      env_file:
        - ./config     